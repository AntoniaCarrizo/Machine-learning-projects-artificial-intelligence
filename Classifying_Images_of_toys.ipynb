{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Classifying Images of toys",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoniaCarrizo/Machine-learning-projects-artificial-intelligence/blob/main/Classifying_Images_of_toys.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYysdyb-CaWM"
      },
      "source": [
        "# Classifying Images of toys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0tMfX2vR0uD"
      },
      "source": [
        "## Install and import dependencies\n",
        "\n",
        "We will use a dataset from TensorFlow Datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7mUJVqcINSM"
      },
      "source": [
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOA5qUl9_-Go"
      },
      "source": [
        "We proceed to import dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbK0Uq7GWaO"
      },
      "source": [
        "# Import Tensorflow\n",
        "import tensorflow as tf\n",
        "# Import TensorFlow Datasets\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfNnMTg7AOpL"
      },
      "source": [
        "To show us the error messages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590z76KRGtKk"
      },
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR0EdgrLCaWR"
      },
      "source": [
        "## Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqDQO0KCaWS"
      },
      "source": [
        "dataset, metadata = tfds.load(name='smallnorb', as_supervised=True, with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtftlQxXDkaB"
      },
      "source": [
        "We divide the dataset into its training and test sub datasets. We will divide our dataset into 24.300 images for training and 24.300 images for testing. We don't need to do this division because the dataset is already divided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHZzHM9PBL4K"
      },
      "source": [
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FDsUlxCaWW"
      },
      "source": [
        "The images are 96 $\\times$ 96 arrays, with pixel values in the range `[0, 255]` (because the images are black and white). The labels are an array of integers, in the range `[0, 4]`. These correspond to the class of toy the image represents:\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>Animals</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Human</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Planes</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Trucks</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Cars</td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjnLH5S2CaWx"
      },
      "source": [
        "class_names = ['Animal', 'Human', 'Plane', 'Truck', 'Car']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brm0b_KACaWX"
      },
      "source": [
        "### Explore the data\n",
        "\n",
        "Let's explore the format of the dataset before training the model. The following shows there are 24.300 images in the training set, and 24.300 images in the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaOTZxFzi48X"
      },
      "source": [
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES6uQoLKCaWr"
      },
      "source": [
        "## Preprocess the data\n",
        "\n",
        "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets.It ensures that each input pixel has a similar data distribution. This makes convergence faster while training the network.\n",
        "\n",
        "By not normalizing the precision drops drastically, the results do not exceed 20% and the predictions are mostly wrong.\n",
        "\n",
        "We will divide each element of training and test by the number of pixels, that is, 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAsH3Zm-76pB"
      },
      "source": [
        "def normalize(images, labels):\n",
        "  images = tf.cast(images, tf.float32)\n",
        "  images /= 255\n",
        "  return images, labels\n",
        "\n",
        "train_dataset =  train_dataset.map(normalize)\n",
        "test_dataset  =  test_dataset.map(normalize)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBtfE3RPBK1"
      },
      "source": [
        "We keep our database in cache, in ram memory. You can train the model more quickly since the model does not have to put the hard disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxreokfDO_Mm"
      },
      "source": [
        "train_dataset =  train_dataset.cache()\n",
        "test_dataset  =  test_dataset.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIQbEiJGXM-q"
      },
      "source": [
        "### Explore the processed data\n",
        "\n",
        "We will analyze the database to understand it a little better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Ob5raXSnAp"
      },
      "source": [
        "As it is a black and white image, it will not have more dimensions, only one. With reshape we take out a couple of brackets. If the image had more colors it could not be done since there is more than one list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSzE9l7PjHx0"
      },
      "source": [
        "\n",
        "image, label = tf.data.experimental.get_single_element(test_dataset.take(1))\n",
        "image = image.numpy().reshape((96,96))\n",
        "# Plot the image\n",
        "plt.figure()\n",
        "plt.imshow(image, cmap=plt.cm.binary)\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee638AlnCaWz"
      },
      "source": [
        "We show some images to verify if the data correspond to the image and that they are in the correct format to build and train the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZTImqg_CaW1"
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "i = 0\n",
        "for (image, label) in test_dataset.take(15):\n",
        "    image = image.numpy().reshape((96,96))\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(image, cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[label])\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59veuiEZCaW4"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxg1XGm0eOBy"
      },
      "source": [
        "### Setup the layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gut8A_7rCaW6"
      },
      "source": [
        "This network has three layers:\n",
        "\n",
        "* **input** `tf.keras.layers.Flatten` — This layer transforms the images from a 2d-array of 96 $\\times$ 96 pixels, to a 1d-array of 9216 pixels. \n",
        "\n",
        "* **\"hidden\"** `tf.keras.layers.Dense`— Two dense layers with 100 neurons\n",
        "\n",
        "* **output**  `tf.keras.layers.Dense` — 5-node * softmax * layer, each node represents a class of toy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op-j73k40cJN"
      },
      "source": [
        "Differences between number of layers:\n",
        "- with 1 hidden layer: the loss function remains constant and does not improve. Acurracy only reaches 19% and category hits are minimal.\n",
        "- with 2 hidden layer: the loss function is decreasing (improving). The acurracy reaches up to 78 %% and the category hits are adequate.\n",
        "- with 3 hidden layers: the sva loss function decreasing (improving). The acurracy decreases and is obtained up to 72%.\n",
        "We conclude that with 2 hidden layer the model works correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6li3dJ118gk"
      },
      "source": [
        "Differences in the number of nurones:\n",
        "- 10 neurons: the loss function stopped improving, it remained constant after the first epoch. The accuracy only reached 19% and the hits in the categories are minimal.\n",
        "- 50 neurons: the loss function was decreasing and the accuracy reached 74%. The number of hits in the categories increased considerably.\n",
        "- 100 neurons: the loss function was decreasing and the accuracy reached 78%. The number of hits in the categories is good.\n",
        "- between 200 and 500 neurons, the accuracy remained in the range between 70% and 75%.\n",
        "\n",
        "The differences between the results of very few neurons and many neurons is due to the fact that:\n",
        "- when using few neurons it is misadjusted, that is, there are few neurons to detect the input data signals.\n",
        "- When using many neurons, overfitting can occur, that is, the amount of information in the training data is not enough to train all the neurons there are.\n",
        "\n",
        "In conclusion, we believe that the number of neurons that obtained the best results was 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqW4TmqIJbKT"
      },
      "source": [
        "Differences in the activation function:\n",
        "Different activation functions were used in the hidden layers to see that we chose the one with the best results.\n",
        "- Relu: Rapid learning, offers much better performance and generalizability in deep learning, all values less than zero are set to zero. 77% acurracy was reached, the categorical_hinge was increasing, the number of hits in some test images is good.\n",
        "- Leaky_relu: Has a small slope for negative values. A lower percentage of acurracy is reached, 73%, however, it works well in the number of hits in test images.\n",
        "- Tanh: Has a range between -1 and 1, the function may produce some dead neurons during the calculation process. The loss function does not improve, it remains constant and the accuracy is very small 20%, the number of hits in some test images is bad.\n",
        "-Elu: It has a limitation: it is not zero-centered, but it is a good alternative to Relu. 77% acurracy was reached, the categorical_hinge was increasing, however, the number of hits in some test images is good.\n",
        "\n",
        "We decided to occupy relu for its good accuracy and number of hits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ODch-OFCaW4"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(96, 96, 1)),\n",
        "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
        "    tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yANoH-K5B5Q8"
      },
      "source": [
        "### Compile the model\n",
        "\n",
        "To compile it we pass the optimizer, the loss function and the metrics (accuracy).\n",
        "The optimizer used is Adam as it is the best for these models. As a loss function we will use SparseCategoricalCrossentropy, we use this loss function because there are two or more kinds of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXJiMcOyRrAw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZufuDvLURlR"
      },
      "source": [
        "Metrics:\n",
        "- accuracy: It helps us since it is good at classification problems. It helps us to evaluate the model since it is the proportion of true results among the total number of cases examined, the higher this number is, it means that there are more correct predictions.\n",
        "- mean_squared_logarithmic_error: see the relative difference between the true and predicted value, or in other words, it only cares about the percentage difference between them. As this value increases, it means that there are better predictions.\n",
        "- categorical_hinge: Computes the categorical hinge metric between y_true and y_pred. As it increases there are better predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhan11blCaW7"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy','mean_squared_logarithmic_error', 'categorical_hinge'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKF6uW-BCaW-"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model\n",
        "- Repeat: repeats the data set as many times as possible.\n",
        "-Shuffle prevents the model from memorizing the images, messes up the images.\n",
        "-Batch: tells fit how many images we will pass through epoch.\n",
        "-Fit: The training is done by calling the model.fit method.\n",
        "\n",
        "As the epoch increases the loss function begins to rise and fall without finding a good result, the more epoch the more the loss function changes. We consider that a good number of epoch for this project is 10 since the function of loss remains in descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_Dp8971McQ1"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvwvpA64CaW_"
      },
      "source": [
        "history=model.fit(train_dataset, epochs=10, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ZVOhugCaXA"
      },
      "source": [
        "As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.80 (or 80%) on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4dP4yyV4CIA"
      },
      "source": [
        "We graph the loss function. The loss function usually goes down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbKcqSlq6WGK"
      },
      "source": [
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel(\"Loss Magnitude\")\n",
        "plt.plot(history.history['loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsoS7CPDCaXH"
      },
      "source": [
        "## Make predictions and explore\n",
        "\n",
        "With the model trained, we can use it to make predictions about some images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccoz4conNCpl"
      },
      "source": [
        "for test_images, test_labels in test_dataset.take(1):\n",
        "  test_images = test_images.numpy()\n",
        "  test_labels = test_labels.numpy()\n",
        "  predictions = model.predict(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl91RPhdCaXI"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Kk1voUCaXJ"
      },
      "source": [
        "The first prediction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DmJEUinCaXK"
      },
      "source": [
        "predictions[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hw1hgeSCaXN"
      },
      "source": [
        "We can see which label has the highest confidence value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsqenuPnCaXO"
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E51yS7iCCaXO"
      },
      "source": [
        "So the model is most confident that this image is a Truck."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd7Pgsu6CaXP"
      },
      "source": [
        "class_names[test_labels[0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygh2yYC972ne"
      },
      "source": [
        "We can graph this to see the complete set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O7KAxX4X6rQ"
      },
      "source": [
        "def plot_image(i, predictions_array, true_labels, images):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.imshow(img[...,0], cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "  \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(np.arange(5), class_names, rotation=45)\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(5), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1]) \n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  \n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Ov9OFDMmOD"
      },
      "source": [
        "We plot the first prediction to see the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV5jw-5HwSmO"
      },
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions, test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgdvGD52CaXR"
      },
      "source": [
        "We observe some predicted images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQlnbqaw2Qu_"
      },
      "source": [
        "num_rows = 6\n",
        "num_cols = 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}