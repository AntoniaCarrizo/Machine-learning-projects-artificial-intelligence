{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classifying Images- Convolutional network .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AntoniaCarrizo/Machine-learning-projects-artificial-intelligence/blob/main/Classifying_Images_Convolutional_network_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBTwC4xIICif"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "267MmkArqKnG"
      },
      "source": [
        "We import the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z2H8WYCST-C",
        "outputId": "f33caa28-6d49-4ad6-bc5d-154b0462e285"
      },
      "source": [
        "!pip install -U tensorflow_datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Collecting tensorflow_datasets\n",
            "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 18.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.62.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.12.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-datasets-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7crmAAOMGUWG"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_datasets as tfds\n",
        "import math\n",
        "tfds.disable_progress_bar()\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmdtPatD57-k"
      },
      "source": [
        "We import the encrypt data set and immediately separate the images for training and the images for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ7raMSbuUf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171d76ca-7aca-42be-ba75-05cd50285e22"
      },
      "source": [
        "dataset, metadata = tfds.load(name='cifar10', as_supervised=True, with_info=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to /root/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n",
            "\u001b[1mDataset cifar10 downloaded and prepared to /root/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbshzg1sGmGW"
      },
      "source": [
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic-GwKe3trb0"
      },
      "source": [
        "#Descriptive data analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhH_eGQZvPCw"
      },
      "source": [
        "We see the amount of data we have for testing and training. In this case it was 50,000 of train and 10,000 of test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XWVx4q8uL8Y",
        "outputId": "32c906e8-fd75-4759-cb5a-90d66bf49423"
      },
      "source": [
        "num_train_examples = metadata.splits['train'].num_examples\n",
        "num_test_examples = metadata.splits['test'].num_examples\n",
        "print(\"Number of training examples: {}\".format(num_train_examples))\n",
        "print(\"Number of test examples:     {}\".format(num_test_examples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 50000\n",
            "Number of test examples:     10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCiUscfIvm9a"
      },
      "source": [
        "The images are 32 $\\times$ 32 arrays, with pixel values in the range `[0, 255]`. The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* :\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <th>Label</th>\n",
        "    <th>Class</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>0</td>\n",
        "    <td>Airplane/top</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>1</td>\n",
        "    <td>Car</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>2</td>\n",
        "    <td>Bird</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>3</td>\n",
        "    <td>Cat</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>4</td>\n",
        "    <td>Deer</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>5</td>\n",
        "    <td>Dog</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>6</td>\n",
        "    <td>Frog</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>7</td>\n",
        "    <td>Horse</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>8</td>\n",
        "    <td>Ship</td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "    <td>9</td>\n",
        "    <td>Truck</td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmKfMkG26fnN"
      },
      "source": [
        "We relate each value of the labels to a word to understand it better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoJC5D5II9m"
      },
      "source": [
        "class_names = ['Airplane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxRLA53Cz-Z6"
      },
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\n",
        "test_dataset = test_dataset.cache().batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt7MsGJc7Iha"
      },
      "source": [
        "We create a preview of a small set of images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RukevKHp1e_"
      },
      "source": [
        "sample_training_images, labels = next(iter(train_dataset))\n",
        "def plotImages(images_arr,labels):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax, lb in zip(images_arr, axes, labels):\n",
        "        ax.imshow(img)\n",
        "        ax.set_xlabel(class_names[lb])       \n",
        "    plt.tight_layout()   \n",
        "    plt.show()\n",
        "plotImages(sample_training_images[:10],labels[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkVzSAoAxBm0"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaBDWJFf7SoI"
      },
      "source": [
        "We see that the range of our images is greater than 1, it is not normalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aznE5l9X6qfV"
      },
      "source": [
        "sample_training_images, labels = next(iter(train_dataset))\n",
        "print(sample_training_images[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVut9O610S8u"
      },
      "source": [
        "To carry out the processing, we only normalize the images, we do not use generators because the dataset is already ready from tensorflow, the images already come with a defined size of 32x32."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2ZPH_Yr2hLi"
      },
      "source": [
        "The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets. It ensures that each input pixel has a similar data distribution. This makes convergence faster while training the network. We will divide each element of training and test by the number of pixels, that is, 255.\n",
        "\n",
        "This is what we do with the following two lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsqKhVoD6Rb9"
      },
      "source": [
        "def normalize(images, labels):\n",
        "  images = tf.cast(images, tf.float32)\n",
        "  images /= 255\n",
        "  return images, labels\n",
        "\n",
        "# The map function applies the normalize function to each element in the train\n",
        "# and test datasets\n",
        "train_dataset =  train_dataset.map(normalize)\n",
        "test_dataset  =  test_dataset.map(normalize)\n",
        "\n",
        "# The first time you use the dataset, the images will be loaded from disk\n",
        "# Caching will keep them in memory, making training faster\n",
        "train_dataset =  train_dataset.cache()\n",
        "test_dataset  =  test_dataset.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y69v29VF7cio"
      },
      "source": [
        "We check that the images are in a range from 0 to 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4fRaSU7Hr0"
      },
      "source": [
        "sample_training_images, labels = next(iter(train_dataset))\n",
        "print(sample_training_images[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j02xO6IoLzNt"
      },
      "source": [
        "# Define convolutional Neural Network mode\n",
        "The model consist of 3 cnn layers with an average pooling on each of them. Then a fully connected layer with a relu activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqbFD4BSCajM"
      },
      "source": [
        "The activation function used is relu, because it is the most common to use in convolutional layers. Rapid learning offers much better performance and generalizability in deep learning, all values less than zero are set to zero.\n",
        "The function used in the output is softmax, because we are working with categories, softmax takes the input values and transforms them into values between 0 and 1, so that they can be interpreted as probabilities.\n",
        "\n",
        "What happens if we change the kernel number?\n",
        "- 3x3:\n",
        "- 5x5:\n",
        "- 7x7:\n",
        "\n",
        "What happens if we change the pooling?\n",
        "- Max pooling:\n",
        "- Average pooling:\n",
        "\n",
        "What happens when adding more number of layers and number of neurons?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGgT98TCL2mK"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=(32, 32, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (5,5), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d--_CCbpEHKm"
      },
      "source": [
        "##Compile the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK5Wa0nyJnbC"
      },
      "source": [
        "Parameters used:\n",
        "- Loss functions: Sparse categorical crossentropy, computes the crossentropy loss between the labels and predictions, it is used when we have two or more categories, in this case we have 10 so it works perfectly for us.\n",
        "- Batch parameters: The optimal value we obtained was 32, since with a higher value, for example 64, the accuracy decreased, and the other metrics were worse than with 32.\n",
        "- Epochs: the highest number that did not cause overfitting was\n",
        "- Optimizer: Adam is generally used because it is the simplest, it achieves good results quickly when having a large amount of data.\n",
        "- Metrics:\n",
        "  * Categorical accuracy: Calculate how often the predictions match the labels.\n",
        "  * Accuracy: It helps us since it is good at classification problems. It helps us to evaluate the model since it is the proportion of true results among the total number of cases examined, the higher this number is, it means that there are more correct predictions.\n",
        "  * Mean absolute error: Calculates the mean absolute error between labels and predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7HC6W6YMwG9"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy', 'CategoricalAccuracy', 'MeanAbsoluteError'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glz4L_LeKrVu"
      },
      "source": [
        "We look at the table of our model, we see that the parameters are increasing more and more due to the '' zoom '' caused by the convolutional layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rF9eC9FMy-l"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wczU3zlULJEZ"
      },
      "source": [
        "##Train the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4a1UmsyLhcb"
      },
      "source": [
        "history=model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=math.ceil(num_train_examples/BATCH_SIZE), validation_data=test_dataset, validation_steps=math.ceil(num_test_examples/BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC4TEcdHM4kh"
      },
      "source": [
        "##Results and statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6_EyHblMqvL"
      },
      "source": [
        "We evaluate the accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8AvjvV_NkCe"
      },
      "source": [
        "We compare how the model works on the test data set. Use all the examples that we have in the test data set to assess the precision. The idea is that the value obtained is close to the accuracy obtained previously, thus we verify that there was no overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8lFpToqNBuF"
      },
      "source": [
        "test_loss, test_accuracy, test_categorical_accuracy, test_mean_absolute_error = model.evaluate(test_dataset, steps=math.ceil(num_test_examples/BATCH_SIZE))\n",
        "print('Accuracy on test dataset:', test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsasAozWOKcN"
      },
      "source": [
        "We show the statistics training and validation accuracy and training and validation loss, this will help us to see the results better, the idea is that these lines are not so different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4xRcYkReDew"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(EPOCHS)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.savefig('./foo.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGK3yPcWPUt"
      },
      "source": [
        "We show some results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQ0RyQG1WR1P"
      },
      "source": [
        "for test_images, test_labels in test_dataset.take(1):\n",
        "  test_images = test_images.numpy()\n",
        "  test_labels = test_labels.numpy()\n",
        "  predictions = model.predict(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekw8FlIIWV6f"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5IJa2wXWbet"
      },
      "source": [
        "def plot_image(i, predictions_array, true_labels, images):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.imshow(img[...,0], cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "  \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1]) \n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  \n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HxSIkwu88yN"
      },
      "source": [
        "We show some images with their corresponding predictions, the images are in black and white to facilitate the code to show these results, however, this does not affect the final results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPk8rkNzWjDc"
      },
      "source": [
        "# Plot the first X test images, their predicted label, and the true label\n",
        "# Color correct predictions in blue, incorrect predictions in red\n",
        "num_rows = 6\n",
        "num_cols = 4\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions, test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0R5a4eI-oKU"
      },
      "source": [
        "##Comparing the network previous and post dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ihrsKV_XTP"
      },
      "source": [
        "##Test model with requested image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhVz0bleMY5U"
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKmK0g7SNRSb"
      },
      "source": [
        "We mount drive to get the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUPZoiOqDo1a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er92AxQNE5o4"
      },
      "source": [
        "img_path = '/content/drive/MyDrive/Colab Notebooks/sample_image-1.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP_nVs-6LsQO"
      },
      "source": [
        "original_img = mpimg.imread(img_path)[:,:,:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUp8g82nTY_7"
      },
      "source": [
        "See if the image is normalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khr7bzlJSjUU"
      },
      "source": [
        "print(original_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1QoJTHQM2N9"
      },
      "source": [
        "plt.imshow(original_img, interpolation='none')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LoITrITjlc"
      },
      "source": [
        "We modify the size to be 32x32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS9tEENCPjh2"
      },
      "source": [
        "res = cv2.resize(original_img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6u9PlCJTkqA"
      },
      "source": [
        "We make the prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QM9grfqVM-sA"
      },
      "source": [
        "test = np.array([original_img])\n",
        "prediction = model.predict(np.array([res]))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATVeS_PrUzee"
      },
      "source": [
        "prediction.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMNm6gWzVVZM"
      },
      "source": [
        "np.argmax(prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKu9ve3EVYIr"
      },
      "source": [
        "class_names[np.argmax(prediction)]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}